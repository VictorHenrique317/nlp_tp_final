{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (2.15.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: multiprocess in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: pandas in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: aiohttp in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: packaging in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: xxhash in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: filelock in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from huggingface-hub>=0.18.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.26.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/vip/victor.henrique/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 16:43:27.210600: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 16:43:30.551690: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-21 16:43:30.553108: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-21 16:43:38.484177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/home/vip/victor.henrique/.local/lib/python3.10/site-packages/keras/preprocessing/sequence.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m \u001b[39mimport\u001b[39;00m PorterStemmer, WordNetLemmatizer\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m Tokenizer\n\u001b[0;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'pad_sequences' from 'keras.preprocessing.sequence' (/home/vip/victor.henrique/.local/lib/python3.10/site-packages/keras/preprocessing/sequence.py)"
     ]
    }
   ],
   "source": [
    "%pip install datasets\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vip/victor.henrique/.local/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"talanAI/resumesamples\")\n",
    "dataset = pd.DataFrame(dataset['train'])\n",
    "dataset = dataset.dropna()\n",
    "dataset = dataset[:100]\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vip/victor.henrique/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     hr administr market associ hr administr summar...\n",
      "1     hr specialist us hr oper summari versatil medi...\n",
      "2     hr director summari 20 year experi recruit 15 ...\n",
      "3     hr specialist summari dedic driven dynam 20 ye...\n",
      "4     hr manag skill highlight hr skill hr depart st...\n",
      "                            ...                        \n",
      "95    director hr execut profil ambiti human resourc...\n",
      "96    hr senior specialist career overview dedic ser...\n",
      "97    hr custom servic repr summari excel team playe...\n",
      "98    hr servic repr summari multi skill profess goo...\n",
      "99    hr assist intern summari new graduat seek work...\n",
      "Name: Resume, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # Lowercase the text\n",
    "    text = re.sub(r'\\W', ' ', text) # Remove punctuation and special characters\n",
    "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text) # Remove single characters\n",
    "    text = re.sub(r'\\s+', ' ', text) # Remove multiple spaces\n",
    "    text = [stemmer.stem(word) for word in text.split() if word not in stop_words] # Remove stopwords and stem the words\n",
    "    return ' '.join(text)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "dataset['Resume'] = dataset['Resume'].apply(preprocess_text)\n",
    "print(dataset['Resume'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
